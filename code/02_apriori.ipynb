{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce07370",
   "metadata": {},
   "source": [
    "## 1. Project Header & Path Setup\n",
    "In the first cell, add the logic to link your code folder so you can access your time_operation decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820f9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Identify project structure\n",
    "project_root = Path.cwd().parent\n",
    "code_path = project_root / \"code\"\n",
    "results_dir = project_root / \"results\"\n",
    "\n",
    "# Add code folder to path to import utils\n",
    "if str(code_path) not in sys.path:\n",
    "    sys.path.append(str(code_path))\n",
    "\n",
    "from utils import time_operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643f223",
   "metadata": {},
   "source": [
    "## 2. Loading the Baskets\n",
    "Use a timed function to load your DataFrames. This helps document the I/O overhead in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abe4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrames in 284.18 ms\n"
     ]
    }
   ],
   "source": [
    "@time_operation\n",
    "def load_checkpoints():\n",
    "    df_user = pd.read_pickle(results_dir / \"user_baskets.pkl\")\n",
    "    df_session = pd.read_pickle(results_dir / \"session_baskets.pkl\")\n",
    "    return df_user, df_session\n",
    "\n",
    "(df_user, df_session), load_time = load_checkpoints()\n",
    "print(f\"Loaded DataFrames in {load_time:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c21ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2352 entries, 0 to 2351\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   user_id  2352 non-null   int32 \n",
      " 1   basket   2352 non-null   object\n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 27.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20772 entries, 0 to 20771\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   user_id  20772 non-null  int32         \n",
      " 1   date     20772 non-null  datetime64[us]\n",
      " 2   basket   20772 non-null  object        \n",
      "dtypes: datetime64[us](1), int32(1), object(1)\n",
      "memory usage: 405.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_user.info()\n",
    "df_session.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user.sample(10)\n",
    "df_session.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ca3c3",
   "metadata": {},
   "source": [
    "## 3. Transaction Encoding (One-Hot Encoding)\n",
    "The mlxtend algorithms require a boolean matrix. We will wrap this in your timing decorator to show the cost of data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cace8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User matrix shape: (2352, 112) (Encoded in 46.50 ms)\n",
      "Session matrix shape: (20772, 112) (Encoded in 240.97 ms)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "@time_operation\n",
    "def encode_data(baskets):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(baskets).transform(baskets)\n",
    "    return pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Encode both levels\n",
    "user_encoded, u_enc_time = encode_data(df_user['basket'])\n",
    "session_encoded, s_enc_time = encode_data(df_session['basket'])\n",
    "\n",
    "print(f\"User matrix shape: {user_encoded.shape} (Encoded in {u_enc_time:.2f} ms)\")\n",
    "print(f\"Session matrix shape: {session_encoded.shape} (Encoded in {s_enc_time:.2f} ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f659e4",
   "metadata": {},
   "source": [
    "## 4. Apriori Parameter Fine-Tuning\n",
    "We will iterate through different support and confidence thresholds. The goal is to find the **elbow point**â€”the settings that provide enough rules to be insightful without generating thousands of redundant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e985e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "from utils import time_operation\n",
    "\n",
    "# Define the tuning grid\n",
    "support_levels = [0.05, 0.03, 0.02, 0.01]\n",
    "confidence_levels = [0.5, 0.6, 0.7]\n",
    "tuning_results = []\n",
    "\n",
    "print(\"--- Starting Apriori Fine-Tuning (Session Level) ---\")\n",
    "\n",
    "for supp in support_levels:\n",
    "    # Measure Frequent Itemset generation time\n",
    "    itemsets, duration = time_operation(apriori)(session_encoded, min_support=supp, use_colnames=True)\n",
    "    \n",
    "    for conf in confidence_levels:\n",
    "        rules = association_rules(itemsets, metric=\"confidence\", min_threshold=conf)\n",
    "        \n",
    "        tuning_results.append({\n",
    "            \"Algorithm\": \"Apriori\",\n",
    "            \"Support\": supp,\n",
    "            \"Confidence\": conf,\n",
    "            \"Time_ms\": duration,\n",
    "            \"Rule_Count\": len(rules),\n",
    "            \"Avg_Lift\": rules['lift'].mean() if not rules.empty else 0\n",
    "        })\n",
    "\n",
    "# Save tuning log for the report table\n",
    "df_apriori_log = pd.DataFrame(tuning_results)\n",
    "df_apriori_log.to_csv(results_dir / \"apriori_tuning_log.csv\", index=False)\n",
    "df_apriori_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c312570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
