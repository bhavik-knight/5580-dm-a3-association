\section{Theoretical Framework}
\label{sec:theoretical_framework}

Association Rule Mining (ARM) is an \textbf{unsupervised learning} technique for discovering hidden relationships in large datasets. Unlike supervised learning, ARM identifies local structures by uncovering sets of items that frequently co-occur. While ARM defines the objective, the \textbf{Apriori Algorithm} is the classical methodology used to solve this task efficiently.

\subsection{The Apriori Algorithm}
The Apriori algorithm uses a \textit{level-wise search}, leveraging frequent $k$-itemsets to explore $(k+1)$-candidates. Its efficiency stems from pruning the search space before database support counting.

\subsubsection{Join-and-Prune Logic}
The algorithm iterates through two primary steps at each level $k$:
\begin{enumerate}
\item \textbf{Join Step ($L_{k-1} \Join L_{k-1}$):} Generates candidate $k$-itemsets ($C_k$) by joining frequent $(k-1)$-itemsets ($L_{k-1}$) with themselves.
\item \textbf{Pruning Step:} For any candidate in $C_k$, the algorithm evaluates its $(k-1)$-subsets. If any subset is missing from $L_{k-1}$, the candidate is pruned; by the Downward Closure Property, it cannot be frequent \cite{geeksforgeeks_apriori}.
\end{enumerate}

\subsection{Key Metrics in ARM}
The significance of discovered rules is quantified using three mathematical pillars:
\begin{itemize}
\item \textbf{Support ($\sigma$):} Frequency of an itemset in the total transaction population.
$$ \sigma(A \rightarrow B) = P(A \cup B) = \frac{\text{Count}(A \cup B)}{\text{Total Transactions}} $$
\item \textbf{Confidence ($\gamma$):} Probability the consequent $B$ appears given the antecedent $A$.
\[ \gamma(A \rightarrow B) = P(B|A) = \frac{\sigma(A \cup B)}{\sigma(A)} \]

\item \textbf{Lift ($L$):} Strength of a rule by comparing observed support to expected support if $A$ and $B$ were independent.
\[ L(A \rightarrow B) = \frac{\gamma(A \rightarrow B)}{\sigma(B)} \]
$L > 1$ indicates a positive correlation, while $L = 1$ suggests item independence.
\end{itemize}